{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Institutional Pattern Discovery with Unsupervised Learning\n",
    "\n",
    "## Course 3: Advanced Classification Models for Student Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening Narrative\n",
    "\n",
    "> *\"We've spent this course predicting who will leave. Now we ask a different question:\n",
    "> **What natural groupings exist among our students \u2014 and what do those patterns tell us\n",
    "> about how to serve them?**\"*\n",
    "\n",
    "### The Provost's Question\n",
    "\n",
    "Imagine your Provost says:\n",
    "\n",
    "> *\"We've been treating all first-year students the same in our retention programs.\n",
    "> But intuitively, we know there are different kinds of at-risk students \u2014\n",
    "> some struggle academically, some financially, some socially.\n",
    "> Can we **discover** these groups from the data itself,\n",
    "> rather than imposing categories from above?\"*\n",
    "\n",
    "**That question is the heart of unsupervised learning.**\n",
    "\n",
    "Unlike supervised learning (Modules 1\u20134), where we predicted a known target (`DEPARTED`),\n",
    "unsupervised learning has **no target variable**. Instead, we let the data reveal\n",
    "its own structure \u2014 clusters of similar students, latent dimensions that compress\n",
    "dozens of variables into interpretable factors, or hidden patterns that no one\n",
    "thought to look for.\n",
    "\n",
    "### What You Will Learn\n",
    "\n",
    "| Concept | What It Does | Institutional Use |\n",
    "|:--------|:-------------|:------------------|\n",
    "| **K-Means Clustering** | Groups similar observations into k clusters | Segment students into distinct risk/behavior profiles |\n",
    "| **PCA** (Principal Component Analysis) | Reduces many variables to a few key dimensions | Simplify complex student data for visualization and insight |\n",
    "| **Elbow Method & Silhouette Score** | Determine optimal number of clusters | Justify the number of student segments to leadership |\n",
    "| **Cluster Profiling** | Describe what makes each cluster unique | Translate data patterns into actionable advising strategies |\n",
    "\n",
    "### Key Difference from Supervised Learning\n",
    "\n",
    "| | Supervised (Modules 1\u20134) | Unsupervised (This Module) |\n",
    "|:--|:------------------------|:--------------------------|\n",
    "| **Goal** | Predict a known outcome | Discover hidden structure |\n",
    "| **Target variable** | Yes (`DEPARTED`) | No |\n",
    "| **Evaluation** | AUC, F1, Accuracy | Silhouette, Inertia, Interpretability |\n",
    "| **Question** | \"Will this student leave?\" | \"What *kinds* of students do we have?\" |\n",
    "| **Action** | Flag individual students | Design group-level interventions |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Visualization defaults\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"All libraries loaded successfully!\")\n",
    "print(\"This module uses matplotlib/seaborn for static, publication-ready visuals.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Case Study 1: Student Entry Segmentation\n",
    "\n",
    "### The Provost's Question\n",
    "\n",
    "> *\"We have 1,000 incoming first-year students with their high school records\n",
    "> and demographic indicators. Can we identify **natural groupings**\n",
    "> among these students to tailor our orientation and advising programs?\"*\n",
    "\n",
    "### Variables (7)\n",
    "\n",
    "| Variable | Description |\n",
    "|:---------|:------------|\n",
    "| `HS_GPA` | High school GPA (2.0\u20134.0) |\n",
    "| `HS_MATH_GPA` | High school math GPA (1.5\u20134.0) |\n",
    "| `HS_ENGL_GPA` | High school English GPA (1.5\u20134.0) |\n",
    "| `UNITS_ATTEMPTED_1` | Units attempted in first semester (9\u201318) |\n",
    "| `FIRST_GEN` | First-generation status (0/1) |\n",
    "| `PELL_ELIGIBLE` | Pell Grant eligibility (0/1) |\n",
    "| `DISTANCE_FROM_CAMPUS` | Miles from campus (1\u2013200) |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Generate synthetic student entry data ---\n",
    "n = 1000\n",
    "\n",
    "entry_df = pd.DataFrame({\n",
    "    'HS_GPA':             np.random.normal(3.2, 0.45, n).clip(2.0, 4.0),\n",
    "    'HS_MATH_GPA':        np.random.normal(3.0, 0.55, n).clip(1.5, 4.0),\n",
    "    'HS_ENGL_GPA':        np.random.normal(3.1, 0.50, n).clip(1.5, 4.0),\n",
    "    'UNITS_ATTEMPTED_1':  np.random.choice([9, 12, 13, 14, 15, 16, 17, 18], n,\n",
    "                                            p=[0.05, 0.15, 0.10, 0.15, 0.25, 0.15, 0.10, 0.05]),\n",
    "    'FIRST_GEN':          np.random.binomial(1, 0.42, n),\n",
    "    'PELL_ELIGIBLE':      np.random.binomial(1, 0.38, n),\n",
    "    'DISTANCE_FROM_CAMPUS': np.random.exponential(25, n).clip(1, 200).round(1)\n",
    "})\n",
    "\n",
    "print(f\"Dataset: {entry_df.shape[0]:,} students \u00d7 {entry_df.shape[1]} variables\")\n",
    "entry_df.describe().round(2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scale the Data\n",
    "\n",
    "**Why scale?** K-Means uses Euclidean distance. Without scaling, variables with larger ranges\n",
    "(like `DISTANCE_FROM_CAMPUS` at 1\u2013200) would dominate over variables with smaller ranges\n",
    "(like `FIRST_GEN` at 0\u20131). `StandardScaler` puts all variables on the same footing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "scaler = StandardScaler()\n",
    "entry_scaled = scaler.fit_transform(entry_df)\n",
    "\n",
    "print(\"Before scaling:\")\n",
    "print(f\"  HS_GPA range:    {entry_df['HS_GPA'].min():.1f} \u2013 {entry_df['HS_GPA'].max():.1f}\")\n",
    "print(f\"  DISTANCE range:  {entry_df['DISTANCE_FROM_CAMPUS'].min():.1f} \u2013 {entry_df['DISTANCE_FROM_CAMPUS'].max():.1f}\")\n",
    "print(f\"\\nAfter scaling (mean \u2248 0, std \u2248 1):\")\n",
    "print(f\"  HS_GPA range:    {entry_scaled[:, 0].min():.2f} \u2013 {entry_scaled[:, 0].max():.2f}\")\n",
    "print(f\"  DISTANCE range:  {entry_scaled[:, 6].min():.2f} \u2013 {entry_scaled[:, 6].max():.2f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Find the Optimal Number of Clusters\n",
    "\n",
    "We use two complementary methods:\n",
    "\n",
    "1. **Elbow Method** \u2014 Plot inertia (within-cluster sum of squares) vs. k. Look for the \"elbow\" where adding more clusters stops helping much.\n",
    "2. **Silhouette Score** \u2014 Measures how similar each point is to its own cluster vs. other clusters. Ranges from -1 to +1; higher is better."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Elbow method + Silhouette scores\n",
    "K_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, n_init=10, random_state=RANDOM_STATE)\n",
    "    labels = km.fit_predict(entry_scaled)\n",
    "    inertias.append(km.inertia_)\n",
    "    silhouettes.append(silhouette_score(entry_scaled, labels))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elbow plot\n",
    "axes[0].plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Clusters (k)')\n",
    "axes[0].set_ylabel('Inertia (Within-Cluster SS)')\n",
    "axes[0].set_title('Elbow Method')\n",
    "axes[0].axvline(x=4, color='red', linestyle='--', alpha=0.7, label='k = 4')\n",
    "axes[0].legend()\n",
    "\n",
    "# Silhouette plot\n",
    "axes[1].plot(K_range, silhouettes, 'go-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Analysis')\n",
    "axes[1].axvline(x=4, color='red', linestyle='--', alpha=0.7, label='k = 4')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"\\nSilhouette scores: \" + \", \".join([f\"k={k}: {s:.3f}\" for k, s in zip(K_range, silhouettes)]))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Fit K-Means with k=4"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "km_entry = KMeans(n_clusters=4, n_init=10, random_state=RANDOM_STATE)\n",
    "entry_df['Cluster'] = km_entry.fit_predict(entry_scaled)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_score(entry_scaled, entry_df['Cluster']):.3f}\")\n",
    "print(f\"\\nCluster Sizes:\")\n",
    "print(entry_df['Cluster'].value_counts().sort_index())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: PCA for Visualization\n",
    "\n",
    "With 7 variables, we can't visualize the clusters directly. PCA compresses the data\n",
    "into 2 dimensions that capture the most variance, letting us *see* the clusters."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pca = PCA(n_components=2, random_state=RANDOM_STATE)\n",
    "entry_pca = pca.fit_transform(entry_scaled)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "scatter = ax.scatter(entry_pca[:, 0], entry_pca[:, 1],\n",
    "                     c=entry_df['Cluster'], cmap='Set2', alpha=0.6, s=30, edgecolors='w', linewidth=0.3)\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "ax.set_title('Student Entry Segments (PCA Projection)')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"PC1 explains {pca.explained_variance_ratio_[0]:.1%} of variance\")\n",
    "print(f\"PC2 explains {pca.explained_variance_ratio_[1]:.1%} of variance\")\n",
    "print(f\"Together: {sum(pca.explained_variance_ratio_[:2]):.1%}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Cluster Profiling\n",
    "\n",
    "This is the most important step for institutional use. We compute the mean of each\n",
    "variable within each cluster, then **interpret what makes each group distinctive**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "profile = entry_df.groupby('Cluster').mean().round(2)\n",
    "profile['Count'] = entry_df['Cluster'].value_counts().sort_index()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CLUSTER PROFILES \u2014 Student Entry Segmentation\")\n",
    "print(\"=\" * 70)\n",
    "print(profile.to_string())\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Heatmap of cluster profiles (z-scored for comparison)\n",
    "profile_z = (profile.drop(columns='Count') - profile.drop(columns='Count').mean()) / profile.drop(columns='Count').std()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.heatmap(profile_z, annot=profile.drop(columns='Count').values, fmt='.2f',\n",
    "            cmap='RdYlGn', center=0, linewidths=1, ax=ax)\n",
    "ax.set_title('Cluster Profiles (color = z-score, numbers = raw means)')\n",
    "ax.set_ylabel('Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation for Stakeholders\n",
    "\n",
    "After examining the cluster profiles, you would write institutional interpretations like:\n",
    "\n",
    "| Cluster | Label | Key Characteristics | Suggested Intervention |\n",
    "|:--------|:------|:-------------------|:----------------------|\n",
    "| 0 | **Prepared Commuters** | High GPA, lives far from campus | Flexible scheduling, online office hours |\n",
    "| 1 | **At-Risk First-Gen** | Lower GPA, first-gen, Pell-eligible | Intensive advising, bridge programs |\n",
    "| 2 | **Strong Local Students** | High GPA, lives close, full load | Honors programs, research opportunities |\n",
    "| 3 | **Moderate & Uncertain** | Average GPA, moderate distance | Mentoring, exploratory advising |\n",
    "\n",
    "> **Note**: Exact labels depend on your data. The key is translating cluster statistics into\n",
    "> language that advisors and administrators can act on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Case Study 2: Program Portfolio Segmentation\n",
    "\n",
    "### The Dean's Question\n",
    "\n",
    "> *\"We have 300 academic programs across the university. Can we identify\n",
    "> which programs are thriving, which are struggling, and which are in between \u2014\n",
    "> based on enrollment, outcomes, and efficiency data?\"*\n",
    "\n",
    "### Variables (6)\n",
    "\n",
    "| Variable | Description |\n",
    "|:---------|:------------|\n",
    "| `ENROLLMENT` | Total headcount (50\u20132000) |\n",
    "| `RETENTION_RATE` | First-year retention (0.40\u20130.98) |\n",
    "| `GRADUATION_RATE` | 6-year grad rate (0.20\u20130.90) |\n",
    "| `FACULTY_RATIO` | Student-to-faculty ratio (8\u201340) |\n",
    "| `COST_PER_STUDENT` | Annual cost per student ($5k\u2013$50k) |\n",
    "| `JOB_PLACEMENT_RATE` | Employment within 1 year (0.50\u20130.98) |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Generate synthetic program portfolio data ---\n",
    "n_prog = 300\n",
    "\n",
    "prog_df = pd.DataFrame({\n",
    "    'ENROLLMENT':        np.random.lognormal(5.5, 0.7, n_prog).clip(50, 2000).astype(int),\n",
    "    'RETENTION_RATE':    np.random.beta(8, 3, n_prog).clip(0.40, 0.98).round(3),\n",
    "    'GRADUATION_RATE':   np.random.beta(5, 4, n_prog).clip(0.20, 0.90).round(3),\n",
    "    'FACULTY_RATIO':     np.random.normal(22, 7, n_prog).clip(8, 40).round(1),\n",
    "    'COST_PER_STUDENT':  np.random.lognormal(9.5, 0.5, n_prog).clip(5000, 50000).round(0),\n",
    "    'JOB_PLACEMENT_RATE': np.random.beta(7, 3, n_prog).clip(0.50, 0.98).round(3)\n",
    "})\n",
    "\n",
    "print(f\"Dataset: {prog_df.shape[0]} programs \u00d7 {prog_df.shape[1]} variables\")\n",
    "prog_df.describe().round(2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Scale \u2192 Elbow + Silhouette \u2192 Fit K-Means \u2192 PCA \u2192 Profile\n",
    "scaler_prog = StandardScaler()\n",
    "prog_scaled = scaler_prog.fit_transform(prog_df)\n",
    "\n",
    "# Optimal k\n",
    "inertias_p, sils_p = [], []\n",
    "for k in range(2, 11):\n",
    "    km = KMeans(n_clusters=k, n_init=10, random_state=RANDOM_STATE)\n",
    "    labels = km.fit_predict(prog_scaled)\n",
    "    inertias_p.append(km.inertia_)\n",
    "    sils_p.append(silhouette_score(prog_scaled, labels))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].plot(range(2, 11), inertias_p, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('k'); axes[0].set_ylabel('Inertia'); axes[0].set_title('Elbow Method \u2014 Programs')\n",
    "axes[0].axvline(x=4, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "axes[1].plot(range(2, 11), sils_p, 'go-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('k'); axes[1].set_ylabel('Silhouette'); axes[1].set_title('Silhouette \u2014 Programs')\n",
    "axes[1].axvline(x=4, color='red', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout(); plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Fit with k=4\n",
    "km_prog = KMeans(n_clusters=4, n_init=10, random_state=RANDOM_STATE)\n",
    "prog_df['Cluster'] = km_prog.fit_predict(prog_scaled)\n",
    "\n",
    "# PCA visualization\n",
    "pca_prog = PCA(n_components=2, random_state=RANDOM_STATE)\n",
    "prog_pca = pca_prog.fit_transform(prog_scaled)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "scatter = ax.scatter(prog_pca[:, 0], prog_pca[:, 1],\n",
    "                     c=prog_df['Cluster'], cmap='Set2', alpha=0.6, s=40, edgecolors='w', linewidth=0.3)\n",
    "ax.set_xlabel(f'PC1 ({pca_prog.explained_variance_ratio_[0]:.1%} variance)')\n",
    "ax.set_ylabel(f'PC2 ({pca_prog.explained_variance_ratio_[1]:.1%} variance)')\n",
    "ax.set_title('Program Portfolio Segments (PCA)')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Profile\n",
    "profile_p = prog_df.groupby('Cluster').mean().round(2)\n",
    "profile_p['Count'] = prog_df['Cluster'].value_counts().sort_index()\n",
    "print(\"=\" * 80)\n",
    "print(\"CLUSTER PROFILES \u2014 Program Portfolio Segmentation\")\n",
    "print(\"=\" * 80)\n",
    "print(profile_p.to_string())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation for the Dean\n",
    "\n",
    "| Cluster | Label | Characteristics | Action |\n",
    "|:--------|:------|:---------------|:-------|\n",
    "| 0 | **High-Performing Flagships** | High enrollment, high outcomes, good placement | Invest and scale |\n",
    "| 1 | **Efficient Small Programs** | Small but strong retention and placement | Protect from cuts |\n",
    "| 2 | **Under-Performing, High-Cost** | Low outcomes, high cost per student | Restructure or sunset |\n",
    "| 3 | **Growing Programs** | Rising enrollment, moderate outcomes | Invest in faculty, support |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Case Study 3: Student Pathway Segmentation\n",
    "\n",
    "### The Academic Affairs Question\n",
    "\n",
    "> *\"Can we identify distinct **behavioral pathways** among students based on their\n",
    "> first-year academic behavior \u2014 course load, grades, DFW rates \u2014 to design\n",
    "> targeted second-year interventions?\"*\n",
    "\n",
    "### Variables (5)\n",
    "\n",
    "| Variable | Description |\n",
    "|:---------|:------------|\n",
    "| `GPA_1` | First-semester GPA |\n",
    "| `GPA_2` | Second-semester GPA |\n",
    "| `DFW_RATE_1` | First-semester DFW rate |\n",
    "| `DFW_RATE_2` | Second-semester DFW rate |\n",
    "| `UNITS_COMPLETED_RATIO` | Proportion of attempted units completed |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Generate synthetic pathway data ---\n",
    "n_path = 1000\n",
    "\n",
    "pathway_df = pd.DataFrame({\n",
    "    'GPA_1':                 np.random.normal(2.8, 0.7, n_path).clip(0.0, 4.0).round(2),\n",
    "    'GPA_2':                 np.random.normal(2.9, 0.65, n_path).clip(0.0, 4.0).round(2),\n",
    "    'DFW_RATE_1':            np.random.beta(2, 8, n_path).clip(0.0, 1.0).round(3),\n",
    "    'DFW_RATE_2':            np.random.beta(2, 8, n_path).clip(0.0, 1.0).round(3),\n",
    "    'UNITS_COMPLETED_RATIO': np.random.beta(8, 2, n_path).clip(0.3, 1.0).round(3)\n",
    "})\n",
    "\n",
    "print(f\"Dataset: {pathway_df.shape[0]:,} students \u00d7 {pathway_df.shape[1]} variables\")\n",
    "pathway_df.describe().round(2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Full pipeline: Scale \u2192 Elbow/Silhouette \u2192 K-Means \u2192 PCA \u2192 Profile\n",
    "scaler_path = StandardScaler()\n",
    "path_scaled = scaler_path.fit_transform(pathway_df)\n",
    "\n",
    "# Optimal k\n",
    "sils_path = []\n",
    "for k in range(2, 11):\n",
    "    km = KMeans(n_clusters=k, n_init=10, random_state=RANDOM_STATE)\n",
    "    sils_path.append(silhouette_score(path_scaled, km.fit_predict(path_scaled)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(range(2, 11), sils_path, 'go-', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('k'); ax.set_ylabel('Silhouette Score')\n",
    "ax.set_title('Silhouette Analysis \u2014 Student Pathways')\n",
    "ax.axvline(x=4, color='red', linestyle='--', alpha=0.7, label='k = 4')\n",
    "ax.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Fit k=4\n",
    "km_path = KMeans(n_clusters=4, n_init=10, random_state=RANDOM_STATE)\n",
    "pathway_df['Cluster'] = km_path.fit_predict(path_scaled)\n",
    "\n",
    "# PCA + scatter\n",
    "pca_path = PCA(n_components=2, random_state=RANDOM_STATE)\n",
    "path_pca = pca_path.fit_transform(path_scaled)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "scatter = ax.scatter(path_pca[:, 0], path_pca[:, 1],\n",
    "                     c=pathway_df['Cluster'], cmap='Set2', alpha=0.6, s=30, edgecolors='w', linewidth=0.3)\n",
    "ax.set_xlabel(f'PC1 ({pca_path.explained_variance_ratio_[0]:.1%})')\n",
    "ax.set_ylabel(f'PC2 ({pca_path.explained_variance_ratio_[1]:.1%})')\n",
    "ax.set_title('Student Pathway Segments (PCA)')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Profile\n",
    "profile_path = pathway_df.groupby('Cluster').mean().round(3)\n",
    "profile_path['Count'] = pathway_df['Cluster'].value_counts().sort_index()\n",
    "print(\"=\" * 70)\n",
    "print(\"CLUSTER PROFILES \u2014 Student Pathways\")\n",
    "print(\"=\" * 70)\n",
    "print(profile_path.to_string())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation for Academic Affairs\n",
    "\n",
    "| Cluster | Label | Characteristics | Intervention |\n",
    "|:--------|:------|:---------------|:-------------|\n",
    "| 0 | **Steady Performers** | Consistent GPA, low DFW, high completion | Light-touch advising |\n",
    "| 1 | **Improving Trajectory** | GPA rises semester 2, DFW drops | Encourage momentum, mentoring |\n",
    "| 2 | **Declining Trajectory** | GPA drops semester 2, DFW rises | Early alert, intensive advising |\n",
    "| 3 | **Chronically Struggling** | Low GPA both semesters, high DFW | Major intervention, bridge support |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Capstone: Course Bottleneck Detection\n",
    "\n",
    "### The Provost & Curriculum Committee Question\n",
    "\n",
    "> *\"We have data on 1,000 course sections. Some courses seem to act as **bottlenecks** \u2014\n",
    "> high DFW rates, low pass rates, long repeat delays. Can we identify clusters\n",
    "> of problematic courses and create a **Risk Prioritization Index** to guide\n",
    "> curriculum reform?\"*\n",
    "\n",
    "### Variables (13)\n",
    "\n",
    "| Variable | Description |\n",
    "|:---------|:------------|\n",
    "| `ENROLLMENT` | Section enrollment |\n",
    "| `DFW_RATE` | DFW rate for the section |\n",
    "| `PASS_RATE` | Pass rate (C or better) |\n",
    "| `AVG_GPA` | Average grade in the section |\n",
    "| `REPEAT_RATE` | Proportion of students repeating |\n",
    "| `AVG_REPEAT_DELAY` | Average semesters before repeat |\n",
    "| `SECTION_SIZE` | Number of seats |\n",
    "| `PCT_FIRST_YEAR` | Proportion of first-year students |\n",
    "| `PCT_STEM` | Proportion of STEM majors |\n",
    "| `INSTRUCTOR_RATING` | Student evaluation score |\n",
    "| `PREREQUISITE_COUNT` | Number of prerequisites |\n",
    "| `IS_GATEWAY` | Gateway course flag (0/1) |\n",
    "| `UNITS` | Course unit value |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Generate synthetic course section data ---\n",
    "n_courses = 1000\n",
    "\n",
    "course_df = pd.DataFrame({\n",
    "    'ENROLLMENT':        np.random.lognormal(3.5, 0.5, n_courses).clip(15, 500).astype(int),\n",
    "    'DFW_RATE':          np.random.beta(2, 7, n_courses).clip(0.01, 0.70).round(3),\n",
    "    'PASS_RATE':         np.random.beta(7, 2, n_courses).clip(0.30, 0.99).round(3),\n",
    "    'AVG_GPA':           np.random.normal(2.7, 0.5, n_courses).clip(0.5, 4.0).round(2),\n",
    "    'REPEAT_RATE':       np.random.beta(1.5, 8, n_courses).clip(0.0, 0.50).round(3),\n",
    "    'AVG_REPEAT_DELAY':  np.random.exponential(1.5, n_courses).clip(0.5, 6.0).round(1),\n",
    "    'SECTION_SIZE':      np.random.choice([25, 30, 35, 40, 50, 60, 80, 100, 150, 200], n_courses),\n",
    "    'PCT_FIRST_YEAR':    np.random.beta(3, 5, n_courses).round(3),\n",
    "    'PCT_STEM':          np.random.beta(3, 4, n_courses).round(3),\n",
    "    'INSTRUCTOR_RATING': np.random.normal(3.8, 0.6, n_courses).clip(1.0, 5.0).round(1),\n",
    "    'PREREQUISITE_COUNT': np.random.choice([0, 1, 2, 3, 4, 5], n_courses, p=[0.15, 0.30, 0.25, 0.15, 0.10, 0.05]),\n",
    "    'IS_GATEWAY':        np.random.binomial(1, 0.30, n_courses),\n",
    "    'UNITS':             np.random.choice([1, 2, 3, 4, 5], n_courses, p=[0.05, 0.10, 0.50, 0.30, 0.05])\n",
    "})\n",
    "\n",
    "print(f\"Dataset: {course_df.shape[0]:,} course sections \u00d7 {course_df.shape[1]} variables\")\n",
    "course_df.describe().round(2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Full pipeline: Scale \u2192 Elbow/Silhouette \u2192 K-Means \u2192 PCA \u2192 Profile\n",
    "scaler_c = StandardScaler()\n",
    "course_scaled = scaler_c.fit_transform(course_df)\n",
    "\n",
    "# Optimal k\n",
    "sils_c = []\n",
    "for k in range(2, 11):\n",
    "    km = KMeans(n_clusters=k, n_init=10, random_state=RANDOM_STATE)\n",
    "    sils_c.append(silhouette_score(course_scaled, km.fit_predict(course_scaled)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(range(2, 11), sils_c, 'go-', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('k'); ax.set_ylabel('Silhouette Score')\n",
    "ax.set_title('Silhouette Analysis \u2014 Course Sections')\n",
    "ax.axvline(x=4, color='red', linestyle='--', alpha=0.7, label='k = 4')\n",
    "ax.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Fit k=4\n",
    "km_course = KMeans(n_clusters=4, n_init=10, random_state=RANDOM_STATE)\n",
    "course_df['Cluster'] = km_course.fit_predict(course_scaled)\n",
    "\n",
    "# PCA visualization\n",
    "pca_c = PCA(n_components=2, random_state=RANDOM_STATE)\n",
    "course_pca = pca_c.fit_transform(course_scaled)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "scatter = ax.scatter(course_pca[:, 0], course_pca[:, 1],\n",
    "                     c=course_df['Cluster'], cmap='Set2', alpha=0.6, s=30, edgecolors='w', linewidth=0.3)\n",
    "ax.set_xlabel(f'PC1 ({pca_c.explained_variance_ratio_[0]:.1%})')\n",
    "ax.set_ylabel(f'PC2 ({pca_c.explained_variance_ratio_[1]:.1%})')\n",
    "ax.set_title('Course Section Segments (PCA)')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.tight_layout(); plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cluster Profile\n",
    "profile_c = course_df.groupby('Cluster').mean().round(3)\n",
    "profile_c['Count'] = course_df['Cluster'].value_counts().sort_index()\n",
    "print(\"=\" * 100)\n",
    "print(\"CLUSTER PROFILES \u2014 Course Bottleneck Detection\")\n",
    "print(\"=\" * 100)\n",
    "print(profile_c.to_string())\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Heatmap\n",
    "profile_cz = (profile_c.drop(columns='Count') - profile_c.drop(columns='Count').mean()) / profile_c.drop(columns='Count').std()\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "sns.heatmap(profile_cz, annot=True, fmt='.2f', cmap='RdYlGn_r', center=0, linewidths=1, ax=ax)\n",
    "ax.set_title('Course Cluster Profiles (z-scored \u2014 red = concerning, green = favorable)')\n",
    "ax.set_ylabel('Cluster')\n",
    "plt.tight_layout(); plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk Prioritization Index\n",
    "\n",
    "We create a composite risk score for each course section to help the Provost and\n",
    "Curriculum Committee prioritize which courses to review first.\n",
    "\n",
    "The index combines three weighted factors:\n",
    "- **DFW Rate** (weight: 0.4) \u2014 direct measure of student failure\n",
    "- **Repeat Rate** (weight: 0.3) \u2014 indicates students are stuck\n",
    "- **Inverse Pass Rate** (weight: 0.3) \u2014 lower pass rate = higher risk"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Risk Prioritization Index\n",
    "course_df['RISK_INDEX'] = (\n",
    "    0.4 * (course_df['DFW_RATE'] / course_df['DFW_RATE'].max()) +\n",
    "    0.3 * (course_df['REPEAT_RATE'] / course_df['REPEAT_RATE'].max()) +\n",
    "    0.3 * ((1 - course_df['PASS_RATE']) / (1 - course_df['PASS_RATE']).max())\n",
    ").round(3)\n",
    "\n",
    "# Top 20 highest-risk sections\n",
    "top_risk = course_df.nlargest(20, 'RISK_INDEX')[\n",
    "    ['ENROLLMENT', 'DFW_RATE', 'PASS_RATE', 'REPEAT_RATE', 'IS_GATEWAY', 'RISK_INDEX', 'Cluster']\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP 20 HIGHEST-RISK COURSE SECTIONS\")\n",
    "print(\"=\" * 80)\n",
    "print(top_risk.to_string())\n",
    "\n",
    "# Risk distribution by cluster\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot\n",
    "course_df.boxplot(column='RISK_INDEX', by='Cluster', ax=axes[0])\n",
    "axes[0].set_title('Risk Index by Cluster')\n",
    "axes[0].set_xlabel('Cluster'); axes[0].set_ylabel('Risk Index')\n",
    "plt.sca(axes[0]); plt.title('Risk Index by Cluster')\n",
    "\n",
    "# Histogram\n",
    "for c in sorted(course_df['Cluster'].unique()):\n",
    "    axes[1].hist(course_df[course_df['Cluster'] == c]['RISK_INDEX'],\n",
    "                 alpha=0.5, bins=20, label=f'Cluster {c}')\n",
    "axes[1].set_xlabel('Risk Index'); axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Risk Index Distribution by Cluster')\n",
    "axes[1].legend()\n",
    "plt.tight_layout(); plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Capstone Deliverables\n",
    "\n",
    "In a real institutional analysis, you would deliver:\n",
    "\n",
    "1. **Cluster summary table** \u2014 One row per cluster with mean statistics and interpretation\n",
    "2. **PCA scatter plot** \u2014 Visual proof that clusters are separable\n",
    "3. **Risk Prioritization Index** \u2014 Ranked list of courses needing review\n",
    "4. **Executive memo** \u2014 1-page summary for the Provost (see template below)\n",
    "\n",
    "#### Executive Memo Template\n",
    "\n",
    "> **To:** Provost / Curriculum Committee\n",
    ">\n",
    "> **From:** Institutional Research\n",
    ">\n",
    "> **Re:** Course Bottleneck Analysis \u2014 Risk-Prioritized Findings\n",
    ">\n",
    "> Using K-Means clustering on 13 course-level metrics for 1,000 sections,\n",
    "> we identified four distinct course profiles. Cluster [X] contains the highest-risk\n",
    "> sections (avg DFW rate = [Y]%, avg repeat rate = [Z]%). These [N] sections\n",
    "> account for [P]% of total DFW enrollments despite being only [Q]% of sections.\n",
    ">\n",
    "> **Recommendation:** Prioritize curriculum review for the top 20 sections\n",
    "> identified by the Risk Prioritization Index (attached)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Final Teaching Outcomes\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "| Concept | Key Takeaway |\n",
    "|:--------|:-------------|\n",
    "| **Unsupervised vs. Supervised** | No target variable \u2014 we discover structure, not predict outcomes |\n",
    "| **StandardScaler** | Always scale before K-Means (distance-based) |\n",
    "| **Elbow Method** | Plot inertia vs. k, look for the bend |\n",
    "| **Silhouette Score** | Quantitative measure of cluster quality (higher = better) |\n",
    "| **K-Means** | `KMeans(n_clusters=k).fit_predict(X_scaled)` \u2014 that's it! |\n",
    "| **PCA** | `PCA(n_components=2).fit_transform(X_scaled)` \u2014 for visualization |\n",
    "| **Cluster Profiling** | `df.groupby('Cluster').mean()` \u2014 the institutional insight step |\n",
    "| **Risk Index** | Weighted composite score for prioritization |\n",
    "\n",
    "### The Consistent Pattern\n",
    "\n",
    "Every case study followed the same pipeline:\n",
    "\n",
    "```\n",
    "Scale \u2192 Find k \u2192 Fit K-Means \u2192 PCA Visualization \u2192 Profile \u2192 Interpret\n",
    "```\n",
    "\n",
    "### Key Institutional Applications\n",
    "\n",
    "1. **Student segmentation** \u2014 Tailor advising and retention programs\n",
    "2. **Program portfolio analysis** \u2014 Identify thriving vs. struggling programs\n",
    "3. **Behavioral pathway detection** \u2014 Design targeted year-2 interventions\n",
    "4. **Course bottleneck identification** \u2014 Prioritize curriculum reform\n",
    "\n",
    "### What's Next\n",
    "\n",
    "In the **Special Topics module** (Module 6), you can explore:\n",
    "- Hierarchical clustering (dendrograms)\n",
    "- DBSCAN (density-based clustering)\n",
    "- t-SNE and UMAP (advanced visualization)\n",
    "- LightGBM, CatBoost, and Neural Networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}